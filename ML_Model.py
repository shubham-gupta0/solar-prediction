# -*- coding: utf-8 -*-
"""solar-power-prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Cjr1C8C1z_q8oARKNL5vsWXkJntxl76

## Things to-do
* EDA
    * Dupbication Detection & Remove if any
    * Null Detection & Filling
    * Outlier Detection
    * Visualizations
* Model Building
    * Scaling (if needed)
    * Regression Models (Forward Selection, Backward Elimination, Transformations etc)
    * Neural Networks (Early Stopping,Dropouts etc)
    * Time Series Model (ARIMA, SARIMAA)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Adjusting color palette

colors_blue = ["#132C33", "#264D58", '#17869E', '#51C4D3', '#B4DBE9']
colors_dark = ["#1F1F1F", "#313131", '#636363', '#AEAEAE', '#DADADA']
colors_binary = [colors_dark[2],colors_blue[2]]
sns.palplot(colors_blue)
sns.palplot(colors_dark)
sns.palplot(colors_binary)

df = pd.read_csv('Solar Power Plant Data.csv')

df.head()

df.rename(columns={"Date-Hour(NMT)":"Date"}, inplace=True)
df.columns

df.shape

df.duplicated().sum()

df.info()

df['Date']= pd.to_datetime(df["Date"], format="%d.%m.%Y-%H:%M")
df.head()


import matplotlib.pyplot as plt
import seaborn as sns

idx_df = df.set_index(['Date'])

idx_df.describe().T

idx_df.isnull().sum()

fig, ax = plt.subplots(figsize = (18,18))
sns.heatmap(idx_df.corr(), ax = ax, annot = True)

idx_df.corr()['SystemProduction'].sort_values(ascending=False)

"""We have some correlated features
- SystemProduction - Radiation +079
- Radiation - Sunshine +0,78
- sunshine - relative air -0,61
- SystemProduction- sunshine +0.56
- Radiation - relative air -0.63
- Air temp - radiation +0.54

There might be a chance of multicollinearity so we will check that later
"""

idx_df.columns[1:-1]

plt.figure(figsize = (15,10), tight_layout = True)
for i,feature in enumerate(idx_df.columns[1:]):
    if feature != 'SystemProduction':
        plt.subplot(3,2,i+1)
        sns.histplot(data = idx_df, x =feature,  palette = colors_binary[1], element='step')

## plot graph


plt.xlabel('Date')
plt.ylabel('SystemProduction')
plt.plot(idx_df)

# Casting dates
df['Date_day'] = df['Date'].dt.date
# df['year'] = df['Date'].dt.year => not good, almost all belongs to same year
df['month'] = df['Date'].dt.month
df['day'] = df['Date'].dt.day
df['hour'] = df['Date'].dt.hour
df.head()

df_day = df.iloc[:,1:9].groupby(by='Date_day').mean()
df_day.head(20)

# Daily plot
plt.figure(figsize = (15,10))
daily = sns.lineplot(x='Date_day', y='SystemProduction',data=df_day)

# Convert 'Date' column to datetime format
df['Date'] = pd.to_datetime(df['Date'])

# Drop non-numeric columns if necessary
non_numeric_columns = ['Date']  # Add any other non-numeric columns here
df_numeric = df.drop(non_numeric_columns, axis=1)

# Convert remaining columns to numeric dtype if possible
df_numeric = df_numeric.apply(pd.to_numeric, errors='coerce')  # Convert non-convertible values to NaN

# Check for missing values
print(df_numeric.isnull().sum())

# Now you can try grouping and calculating the mean
hourly_mean = df_numeric.groupby(by=df['Date'].dt.hour).mean()

plt.figure(figsize = (15,10))
# hourly = sns.lineplot(x='hour', y='SystemProduction',data=df.groupby(by='hour').mean())
# hourly.set_xticks(range(len(df.groupby(by='hour').mean())))
hourly = sns.lineplot(x='hour', y='SystemProduction',data=hourly_mean)
hourly.set_xticks(range(len(hourly_mean)))
#hourly.set_xticklabels(set(df['hours']))

"""Seems like daily production is almost normally distributed if we predict hourly"""

# # mean monthly production
# df.groupby(by='month').mean()
# Convert 'Date' column to datetime format
df['month'] = pd.to_datetime(df['month'])

# Drop non-numeric columns if necessary
non_numeric_columns = ['month']  # Add any other non-numeric columns here
df_numeric = df.drop(non_numeric_columns, axis=1)

# Convert remaining columns to numeric dtype if possible
df_numeric = df_numeric.apply(pd.to_numeric, errors='coerce')  # Convert non-convertible values to NaN

# Check for missing values
print(df_numeric.isnull().sum())

# Now you can try grouping and calculating the mean
month_mean = df_numeric.groupby(by=df['month'].dt.month).mean()

plt.figure(figsize = (15,10))
# hourly = sns.lineplot(x='month', y='SystemProduction',data=df.groupby(by='month').mean())
# hourly.set_xticks(range(len(df.groupby(by='month').mean())))
hourly = sns.lineplot(x='month', y='SystemProduction',data=month_mean)
hourly.set_xticks(range(len(month_mean)))

"""## Outlier Analysis

"""

import plotly.express as px

features = ['WindSpeed', 'Sunshine', 'AirPressure', 'Radiation','AirTemperature', 'RelativeAirHumidity', 'SystemProduction']

for f in features:
    fig = px.box(df, x=f"{f}",
             notched=True,
             title=f"Box plot of {f}",
             hover_data=["SystemProduction"]
            )
    fig.show()

q1 = df.quantile(0.25)
q3 = df.quantile(0.75)
IQR = q3 - q1

print(IQR)

IQR = IQR[:7]

IQR

df.shape

df.columns

# for _, f in enumerate(features):
#     print(f"Percentage of Outliers for {f}:")
#     outliers_mask = (df[f] < (q1[f] - IQR[f] * 1.5)) | (df[f] > (q3[f] + IQR[f] * 1.5))
#     percentage_outliers = (outliers_mask.sum() / len(df)) * 100
#     print(round(percentage_outliers, 2))

# does not make sense to drop all outliers, percentage is high  for some features. Further examination => TBD

# df = df[~((df[features] < (q1 - 1.5 * IQR)) |(df[features] > (q3 + 1.5 * IQR))).any(axis=1)]

"""## VIF for Checking Multicollinearity"""

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# set the independent variables to X
X_vif = df[features]
X_vif = sm.add_constant(X_vif)

# VIF dataframe
vif_data = pd.DataFrame()
vif_data["feature"] = X_vif.columns

# calculating VIF for each feature
vif_data["VIF"] = [variance_inflation_factor(X_vif.values, i) for i in range(len(features)+1)]
print(vif_data)

"""VIF values are <5, which means we don't have multicollinearity problem for the dataset.

## Model Building
"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score


from keras.layers import Dense, BatchNormalization, Dropout, LSTM
from keras.models import Sequential
from tensorflow.keras import regularizers
from keras import callbacks

"""### Models Implemented so far:

* Linear Regression
* Statsmodels OLS
* SVR
* Random Forest
* Decision Tree Regressor
* Gradient Booster Regressor
* ANN

### Train - Test Samples Splitting

- Regresyon icin tüm zamansal featureları cıkarıp tahminleyelim, hangi feature etkili, lineer bir etki var mı?
- regresyon assumptionlarını kontrol et
"""

df = df[features]

df.head()

# Train test split

# Dropped date column because we are using generated features for that.
X = df.drop(["SystemProduction"], axis=1)
y = df["SystemProduction"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

X.shape

X

# Scaling features
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""## Checking Linear Regression Assumptions"""

# Linearity
p = sns.pairplot(df, x_vars=['WindSpeed', 'Sunshine', 'AirPressure', 'Radiation','AirTemperature', 'RelativeAirHumidity'], y_vars='SystemProduction', size=7, aspect=0.7)

"""Question: https://stats.stackexchange.com/questions/65900/does-it-make-sense-to-use-a-date-variable-in-a-regression"""

from sklearn.linear_model import LinearRegression

model_lr = LinearRegression()
model_lr.fit(X_train,y_train)
preds_lr = model_lr.predict(X_test)

type(y_test)

def scores(y_test,preds):
#A function to calculate various scores depending on a given test series and prediction output.
#Metrics used: R2, RMSE, MAE, MAPE
    print(f"R2 score {round(r2_score(y_test,preds),2)}")
    print(f"RMSE score {round(np.math.sqrt(mean_squared_error(y_test,preds)),2)}")
    print(f"MAE score {round(mean_absolute_error(y_test,preds),2)}")
    print(f"MAPE score {round(mean_absolute_percentage_error(y_test,preds),2)}")

"""### Mean of residuals should be close to zero"""

preds_lr.shape, y_test.shape

residuals = y_test.values-preds_lr
mean_residuals = np.mean(residuals)
print("Mean of Residuals {}".format(mean_residuals))

scores(y_test,preds_lr)

"""### KDE Plot"""

sns.kdeplot(residuals)

"""## QQ Plot"""

import statsmodels.api as sm

sm.qqplot(residuals)

plt.show()

"""### Check for Homoscedasticity

Homoscedasticity means that the residuals have equal or almost equal variance across the regression line. By plotting the error terms with predicted terms we can check that there should not be any pattern in the error terms.
"""

# p = sns.scatterplot(preds_lr,residuals)
# plt.xlabel('y_pred/predicted values')
# plt.ylabel('Residuals')
# plt.ylim(-10,10)
# plt.xlim(0,26)
# p = sns.lineplot([0,26],[0,0],color='blue')
# p = plt.title('Residuals vs fitted values plot for homoscedasticity check')

"""### Goldfeld Quandt Test
Checking heteroscedasticity : Using Goldfeld Quandt we test for heteroscedasticity.

- Null Hypothesis: Error terms are homoscedastic
- Alternative Hypothesis: Error terms are heteroscedastic.
"""

residuals.shape, X_train.shape

import statsmodels.stats.api as sms
from statsmodels.compat import lzip
name = ['F statistic', 'p-value']
test = sms.het_goldfeldquandt(residuals, X_test)
lzip(name, test)

"""Since p value is less than 0.05 in Goldfeld Quandt Test, we say error terms are heterocedastic

CHECK => https://www.ucl.ac.uk/~uctp41a/b203/lecture9.pdf

Resources for Linear Regression Assumptions => https://www.kaggle.com/code/shrutimechlearn/step-by-step-assumptions-linear-regression

Data does not comply with linear regression assumptions so we will do some transformations to see if we can fit it.

## Transformations for Linear Regression Assumptions
"""

#TBD

"""## Statsmodels OLS"""

import statsmodels.formula.api as smf
model = smf.ols(formula='SystemProduction ~ WindSpeed + Sunshine + AirPressure + Radiation + AirTemperature + RelativeAirHumidity', data=df).fit()

print(model.summary())

# Since radiation and sunshine has highest correlation maybe we can build a smaller regression model using only these and
# add the others one by one => forward selection?

model_2 = smf.ols(formula='SystemProduction ~ Sunshine + Radiation', data=df).fit()
print(model_2.summary())

model_3 = smf.ols(formula='SystemProduction ~ Sunshine + Radiation + AirTemperature', data=df).fit()
print(model_3.summary())

# SVR

from sklearn.svm import SVR
model_svr = SVR(kernel = 'rbf', gamma = 'scale')
model_svr.fit(X_train,y_train.ravel())
preds_svr = model_svr.predict(X_test)

scores(y_test,preds_svr)

# RF Regressor

from sklearn.ensemble import RandomForestRegressor
model_rf = RandomForestRegressor(n_estimators = 500, random_state = 0)
model_rf.fit(X_train, y_train.ravel())
preds_rf= model_rf.predict(X_test)

scores(y_test,preds_rf)

# Decision Tree Regressor

from sklearn.tree import DecisionTreeRegressor
model_dt = DecisionTreeRegressor(random_state = 0)
model_dt.fit(X_train, y_train)
preds_dt= model_dt.predict(X_test)

scores(y_test,preds_dt)

# Gradient Booster Regression

from sklearn.ensemble import GradientBoostingRegressor
model_gbr = GradientBoostingRegressor(random_state=0)
model_gbr.fit(X_train, y_train)
preds_gbr = model_gbr.predict(X_test)

scores(y_test,preds_gbr)

#!pip install lazypredict

# Trying Lazypredict ==> taking forever to run, im gonna code a function to loop through models later #TBD
#from lazypredict.Supervised import LazyRegressor
# fit all models
#reg = LazyRegressor()
#models, predictions = reg.fit(X_train, X_test, y_train, y_test)
#models

"""## PCA"""

from sklearn.decomposition import PCA
pca = PCA()
pca.fit(X)
print(pca.explained_variance_)

# how to pick n_components for PCA
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');

"""=> Looks weird, even with 0-1 components cumulative explained variance is high (?)

### Hyperparameter Tuning for Models

Best performing models in prev step(these are the scores when we have time dimension within training set):
- Random forest (0.74)
- Gradient Boosting Regressor (0.68)

=> After removing the time dimensions, GBR and RF are still somewhat strong models.

As a start, we will try hyperparameter tuning for these models.
"""

# Import libraries for hyperparameter Tuning
from sklearn.model_selection import RandomizedSearchCV

# Arranging parameters for random forest

# number of trees that can exist in random forest, it will try starting from 50 and with 50 increments until 300
n_estimators = [int(n) for n in np.linspace(start=50, stop=150, num=50)]
max_features = ['auto','sqrt']
max_depth = [int(n) for n in np.linspace(start=5, stop=20, num=10)]
min_samples_split = [5,7]
min_samples_leaf = [3,5]

random_grid = {
    'n_estimators': n_estimators,
    'max_features': max_features,
    'max_depth' : max_depth,
    'min_samples_split' : min_samples_split,
    'min_samples_leaf' : min_samples_leaf
}

rf = RandomForestRegressor()

tuned_rf = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100, cv=3, verbose=2, random_state=0)
tuned_rf.fit(X_train, y_train)

# printing best estimators

tuned_rf.best_params_

best_model_rf = RandomForestRegressor(n_estimators=294, min_samples_split=5, min_samples_leaf=3,max_features='sqrt', max_depth=15)
best_model_rf.fit(X_train,y_train)
preds_best = best_model_rf.predict(X_test)

scores(y_test, preds_best)

model_gbr.get_params()

# Hyperparameter tuning for GBR

n_estimators = [int(n) for n in np.linspace(start=50, stop=300, num=50)]
max_depth = [int(n) for n in np.linspace(start=5, stop=15)]
min_samples_split = [2,3,5,7]
min_samples_leaf = [3,4,5]
learning_rate = [0.01, 0.1, 0.2,1]

gbr_grid = {
 'learning_rate': learning_rate,
 'max_depth': max_depth,
 'min_samples_leaf': min_samples_leaf,
 'min_samples_split': min_samples_split,
 'n_estimators': n_estimators
}

gbr = GradientBoostingRegressor()

tuned_gbr = RandomizedSearchCV(estimator=gbr, param_distributions=gbr_grid, n_iter=10, cv=3, verbose=2, random_state=0)
tuned_gbr.fit(X_train, y_train)

tuned_gbr.best_params_

best_gbr = GradientBoostingRegressor(learning_rate = 0.1, max_depth =8, min_samples_leaf = 5, min_samples_split = 7, n_estimators = 269, random_state = 0)
best_gbr.fit(X_train,y_train)
preds_best_gbr = best_gbr.predict(X_test)

scores(y_test,preds_best_gbr)

import joblib

# Define the file path where you want to save the model
model_filename = 'best_gbr.pkl'

# Save the model to the specified file path
joblib.dump(best_gbr, model_filename)

print(f"Model saved as {model_filename}")

"""After hyperparameter tuning we saw improvement for XGboost Regressor but for Random Forest performance didn't change/went worse.

As a next step, we will plot feature importances for the models and then try stacking them.
"""



"""#### Feature Importances for Best Performing Models"""

# Plotting the graph

plt.figure(figsize=(15,10))
(pd.Series(best_gbr.feature_importances_, index=X.columns).sort_values(ascending=False)
   .plot(kind='barh'))

best_model_rf.feature_importances_

X.columns

# Plotting the graph

plt.figure(figsize=(15,10))
(pd.Series(best_model_rf.feature_importances_, index=X.columns).sort_values(ascending=False)
   .plot(kind='barh'))

"""Looks like for both models **Radiation** is a strong estimator. We expected this result since it had a correlation of 78% with dependent variable.

Also sunshine had 58% correlation but it seems less important for the model (?)

# QUESTIONS???

- some people convert time data with trigonometric funtions to detect seasonality (?) should we try that/does it make sense at all?
- transformations on columns to treat bi-modal dist, skewness etc

## Stacked Models & Ensembles
"""

#TBD

"""### PCA (Principal Component Analysis"""

# TBD LATER

"""### Time Series Model

Trying ARIMA on dataset, if we get good results we can try & select better other ARIMA models(SARIMA etc)
"""

#TBD

"""### Artificial Neural Networks

model = Sequential()

model.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = 26))
model.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu'))
model.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))
model.add(Dropout(0.25))
model.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))


opt = adam_v2(learning_rate=0.00009)
model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])

# Train the ANN
history = model.fit(X_train, y_train, batch_size = 32, epochs = 150, callbacks=[early_stopping], validation_split=0.2)
"""

# TBD LATER, model needs to be optimized & run. Only template-starter code is here

"""## Online Learning"""

# TBD LATER, currently doing research on this & reading codes.